## Linear Algebra and Matrices

*   **Linear Equation**:
    `ğšğŸğ± + â€¦ . . +ğšğ§ğ± = ğ›`
*   **Determinant of a 2x2 Matrix**:
    For matrix A = [ a b ; c d ], det(A) = `ad âˆ’ bc`
*   **Determinant of a larger-size Matrix**:
    `ğğğ­(ğ€) = Ïƒ(âˆ’ğŸ)ğ¢+ğ£ğğğ­(ğŒğ¢,ğ£)`
    where ğŒğ¢,ğ£ is a minor of the matrix.
*   **Matrix Inverse (using Adjoint) for a 2x2 Matrix**:
    For matrix A = [ a b ; c d ], ğ€âˆ’ğŸ = `1 / (ad âˆ’ bc) * [ d -b ; -c a ]` if ad - bc â‰  0.
*   **System of Linear Equations in Matrix Form**:
    `ğ€ğ± = ğ›`
    where A is the matrix of coefficients, ğ± is the column vector of variables, and ğ› is the column vector of constants.
*   **Solving System of Linear Equations using Matrix Inverse**:
    ğ€âˆ’ğŸğ€ğ± = ğ€âˆ’ğŸğ› â‡’ ğˆğ± = ğ€âˆ’ğŸğ› â‡’ `ğ± = ğ€âˆ’ğŸğ›`
*   **Eigenvalue Problem**:
    `ğ€ğ¯ = ğ›Œğ¯`
*   **Characteristic Equation (to find Eigenvalues)**:
    `ğğğ­(ğ€ âˆ’ ğ›Œğˆ) = ğŸ`
    where ğˆ is the identity matrix and ğ›Œ represents the eigenvalues.
*   **Eigenvalue Decomposition**:
    `ğ€ = ğ•ğš²ğ•âˆ’ğŸ`
    where ğ€ is the original matrix, ğ• is the matrix of eigenvectors, ğš² is the diagonal matrix of eigenvalues, and ğ•âˆ’ğŸ is the inverse of ğ•.

## Derivatives and Gradients

*   **Limit Definition of the Derivative (for a scalar function)**:
    ğŸâ€²(ğ±) = `ğ¥ğ¢ğ¦ (ğŸ(ğ±+ğ¡) âˆ’ ğŸ(ğ±)) / ğ¡` when the limit exists.
    `ğ¡â†’ğŸ`
*   **Partial Derivative Notation**:
    âˆ‚f / âˆ‚x or fx
*   **Gradient of a Multivariate Function (Scalar-by-Vector)**:
    For a scalar function y with respect to a vector ğ± = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€:
    ğ›»ğ² = âˆ‚ğ² / âˆ‚ğ± = `[ âˆ‚ğ²/âˆ‚ğ±â‚ ; âˆ‚ğ²/âˆ‚ğ±â‚‚ ; ... ; âˆ‚ğ²/âˆ‚ğ±â‚™ ]`
*   **Vector-by-Vector Derivative (Jacobian Matrix)**:
    For a vector function ğŸ: â„â¿ â†’ â„áµ where ğŸ = [fâ‚(ğ±), ..., fáµ(ğ±)]áµ€ and ğ± = [xâ‚, ..., xâ¿]áµ€, the Jacobian matrix Jf is defined as:
    Jf = `[ âˆ‚fâ‚/âˆ‚xâ‚ ... âˆ‚fâ‚/âˆ‚xâ¿ ; âˆ‚fâ‚‚/âˆ‚xâ‚ ... âˆ‚fâ‚‚/âˆ‚xâ¿ ; ... ; âˆ‚fm/âˆ‚xâ‚ ... âˆ‚fm/âˆ‚xâ¿ ]`
*   **Chain Rule (for composite functions)**:
    If y = f(u) and u = g(x), then dy/dx = f'(u) * g'(x), or `[f(g(x))]' = f'(g(x)) * g'(x)`

## Machine Learning Specific Formulae

*   **Perceptron Weighted Sum (Net Weighted Input)**:
    ğ³ = `Ïƒ(wi * xi) + wo`
    where wi are weights, xi are inputs, and wo is the bias.
*   **Softmax Regression Loss Function (Cross-Entropy Loss) for a single sample**:
    â„“(ğ²áµ¢, à·œğ²áµ¢) = `âˆ’ Ïƒ(ğ²áµ¢áµ * log(à·œğ²áµ¢áµ))`
    where C is the number of classes, ğ²áµ¢ is the one-hot encoded true label, and à·œğ²áµ¢áµ is the predicted probability for class k.
*   **Average Loss (Empirical Risk) for Softmax Regression**:
    à·¡ = `âˆ’ (1/n) * Ïƒ(Ïƒ(ğ²áµ¢áµ * log(à·œğ²áµ¢áµ)))`
    where n is the number of samples. This is also referred to as the cost function.
*   **Softmax Regression Objective (Minimizing Empirical Risk)**:
    ğ›‰âˆ— = `ğšğ«ğ ğ¦ğ¢ğ§ ğ“›(ğ°, ğ›)`
    `ğ›‰âˆ—`
    This means finding the parameters (weights ğ° and bias ğ›) that minimize the average log loss.
*   **Convolutional Layer Parameters**:
    ğ“ğ¨ğ­ğšğ¥ ğğšğ«ğšğ¦ğğ­ğğ«ğ¬ = `(F * F * Cáµ¢ + 1) * K`
    where F is the filter size, Cáµ¢ is the number of input channels, and K is the number of output channels (number of filters). The +1 accounts for the bias term for each filter.
*   **Fully Connected Layer Parameters**:
    ğ“ğ¨ğ­ğšğ¥ ğğšğ«ğšğ¦ğğ­ğğ«ğ¬ = `(Input Units + 1) * Output Units`
    where Input Units is the number of neurons in the input layer, Output Units is the number of neurons in the output layer, and +1 accounts for the bias term for each output neuron.
*   **Output Shape Calculation for Convolutional Layer**:
    Wout = `(Win âˆ’ F + 2P) / S + 1`
    Hout = `(Hin âˆ’ F + 2P) / S + 1`
    Cout = `K`
    where Win/Hin is the input width/height, F is the filter size, P is the padding, S is the stride, and K is the number of filters.
*   **Max Pooling (Mathematical Formalization)**:
    Yi,j = `max(Xâ‚š,q)` within an F Ã— F window.
*   **Average Pooling (Mathematical Formalization)**:
    Yi,j = `(1 / FÂ²) * Ïƒ(Xâ‚š,q)` within an F Ã— F window.
*   **Momentum (Gradient Descent Variant)**:
    Velocity update: ğ‘£â‚œ = `ğ›½ * ğ‘£â‚œâ‚‹â‚ + (1 âˆ’ ğ›½) * Î”Wâ‚œ`
    Weight update: ğ‘¤â‚œ = `ğ‘¤â‚œâ‚‹â‚ âˆ’ ğœ‚ * ğ‘£â‚œ`
    where ğ‘£â‚œ is the velocity, ğ›½ is the momentum coefficient, Î”Wâ‚œ is the gradient, ğ‘¤â‚œ is the weight, and ğœ‚ is the learning rate.
*   **Adam (Optimization Algorithm) - Momentum Term**:
    ğ‘šâ‚œ = `ğ›½â‚ * ğ‘šâ‚œâ‚‹â‚ + (1 âˆ’ ğ›½â‚) * Î”Wâ‚œ`
*   **Adam (Optimization Algorithm) - RMSProp Term**:
    ğ‘£â‚œ = `ğ›½â‚‚ * ğ‘£â‚œâ‚‹â‚ + (1 âˆ’ ğ›½â‚‚) * Î”Wâ‚œÂ²`
*   **Adam (Optimization Algorithm) - Bias Correction**:
    à·ğ‘šâ‚œ = `ğ‘šâ‚œ / (1 âˆ’ ğ›½â‚áµ—)`
    à·ğ‘£â‚œ = `ğ‘£â‚œ / (1 âˆ’ ğ›½â‚‚áµ—)`

## Text Representation Formulae

*   **Raw Term Frequency**:
    ğ­ğŸâ‚œ = `ğŸâ‚œ,d` (frequency count of term t in document d)
*   **Log Normalized Term Frequency (tf-weight)**:
    ğ–â‚œ = `{ 1 + log(tfâ‚œ), if tfâ‚œ > 0 ; 0 Otherwise }`
*   **Inverse Document Frequency (IDF)**:
    ğ¢ğğŸâ‚œ = `log(N / dfâ‚œ)`
    where N is the total number of documents and dfâ‚œ is the number of documents containing term t.
*   **TF-IDF Score**:
    ğ–â‚œfâˆ’idf = `tfâ‚œ,d * idfâ‚œ`
*   **Term Frequency (Alternative Formula)**:
    ğ“ğ…â‚œ,d = `(Number of times t appears in d) / (Total numbers of terms in d)`
*   **Inverse Document Frequency (Alternative Formula)**:
    ğˆğƒğ…â‚œ = `logâ‚â‚€(N / dfâ‚œ)` (using base 10 log)
*   **Unigram Language Model Probability**:
    P(wordáµ¢) = `ğ§áµ¢ / N`
    where ğ§áµ¢ is the number of occurrences of wordáµ¢ and N is the total number of word occurrences (tokens).
*   **Bigram Language Model Probability (Conditional)**:
    P(wordáµ¢ | wordáµ¢â‚‹â‚) = `P(wordáµ¢â‚‹â‚, wordáµ¢) / P(wordáµ¢â‚‹â‚)` (Conditional probability)
*   **Bigram Language Model Probability (Frequency-based)**:
    P(wordáµ¢ | wordáµ¢â‚‹â‚) = `ğ§(wordáµ¢â‚‹â‚, wordáµ¢) / ğ§(wordáµ¢â‚‹â‚)` (Frequency of bigram divided by frequency of previous word)

## Evaluation Metrics

*   **Precision**:
    pğ«ğğœğ¢ğ¬ğ¢ğ¨ğ§ = `TP / (TP + FP)`
    where TP is True Positives and FP is False Positives.
*   **Recall**:
    Recall = `TP / (TP + FN)`
    where FN is False Negatives.
*   **F1 Score**:
    ğ…â‚ âˆ’ ğ’ğœğ¨ğ«ğ = `2 * (Precision * Recall) / (Precision + Recall)`
