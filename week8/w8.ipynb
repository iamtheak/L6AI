{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7DGbuYtB1JI",
        "outputId": "bb0f0f9e-1b47-49c3-d084-40ee97ccbeb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function for Text Cleaning:\n",
        "\n",
        "Implement a Helper Function as per Text Preprocessing Notebook and Complete the following pipeline."
      ],
      "metadata": {
        "id": "SxV-QBHp-B6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Text Cleaning Pipeline"
      ],
      "metadata": {
        "id": "B-llg-TI_Drw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification using Machine Learning Models\n"
      ],
      "metadata": {
        "id": "hzMm4-1KCNkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Instructions: Trump Tweet Sentiment Classification\n",
        "\n",
        "1. **Load the Dataset**  \n",
        "   Load the dataset named `\"trump_tweet_sentiment_analysis.csv\"` using `pandas`. Ensure the dataset contains at least two columns: `\"text\"` and `\"label\"`.\n",
        "\n",
        "2. **Text Cleaning and Tokenization**  \n",
        "   Apply a text preprocessing pipeline to the `\"text\"` column. This should include:\n",
        "   - Lowercasing the text  \n",
        "   - Removing URLs, mentions, punctuation, and special characters  \n",
        "   - Removing stopwords  \n",
        "   - Tokenization (optional: stemming or lemmatization)\n",
        "   - \"Complete the above function\"\n",
        "\n",
        "3. **Train-Test Split**  \n",
        "   Split the cleaned and tokenized dataset into **training** and **testing** sets using `train_test_split` from `sklearn.model_selection`.\n",
        "\n",
        "4. **TF-IDF Vectorization**  \n",
        "   Import and use the `TfidfVectorizer` from `sklearn.feature_extraction.text` to transform the training and testing texts into numerical feature vectors.\n",
        "\n",
        "5. **Model Training and Evaluation**  \n",
        "   Import **Logistic Regression** (or any machine learning model of your choice) from `sklearn.linear_model`. Train it on the TF-IDF-embedded training data, then evaluate it using the test set.  \n",
        "   - Print the **classification report** using `classification_report` from `sklearn.metrics`.\n"
      ],
      "metadata": {
        "id": "oFltIxr9L2Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"/content/drive/MyDrive/AI last sem/week8/Copy of trum_tweet_sentiment_analysis.csv\""
      ],
      "metadata": {
        "id": "RKRSlpsizpDS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(dataset_url)"
      ],
      "metadata": {
        "id": "QoXCVd4lzxF6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def removeunwanted_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def stemming(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# --- Text Cleaning Pipeline ---\n",
        "\n",
        "def text_cleaning_pipeline(dataset, rule=\"lemmatize\"):\n",
        "    \"\"\"\n",
        "    Cleans the 'text' column in a DataFrame.\n",
        "    rule: 'lemmatize' or 'stem'\n",
        "    Returns: a Series with cleaned text\n",
        "    \"\"\"\n",
        "    data = dataset[\"text\"].astype(str).str.lower()\n",
        "    data = data.apply(remove_urls)\n",
        "    data = data.apply(remove_emoji)\n",
        "    data = data.apply(removeunwanted_characters)\n",
        "    data = data.apply(remove_punct)\n",
        "\n",
        "    # Tokenize\n",
        "    data = data.apply(lambda x: x.split())\n",
        "\n",
        "    # Remove stopwords\n",
        "    data = data.apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
        "\n",
        "    # Lemmatize or Stem\n",
        "    if rule == \"lemmatize\":\n",
        "        data = data.apply(lemmatization)\n",
        "    elif rule == \"stem\":\n",
        "        data = data.apply(stemming)\n",
        "    else:\n",
        "        raise ValueError(\"Choose rule as either 'lemmatize' or 'stem'\")\n",
        "\n",
        "    # Join back to string\n",
        "    return data.apply(lambda tokens: \" \".join(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zIoyRUw-bS5",
        "outputId": "8fa530bc-fb88-44cd-ccf1-3c54b3241c0d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwRZQzuP4jJF",
        "outputId": "3399078a-a790-44d0-910c-d72621097729"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1850123 entries, 0 to 1850122\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Dtype \n",
            "---  ------     ----- \n",
            " 0   text       object\n",
            " 1   Sentiment  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 28.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(dataset_url)\n",
        "cleaned_text = text_cleaning_pipeline(df, rule=\"lemmatize\")\n",
        "print(cleaned_text.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfxeUPZE-c-o",
        "outputId": "b798fec9-bc47-473e-e7d0-d35fd53096bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    rt johnleguizamo trump draining swamp taxpayer...\n",
            "1    icymi hacker rig fm radio station play antitru...\n",
            "2    trump protest lgbtq rally new york bbcworld vi...\n",
            "3    hi im pier morgan david beckham awful donald t...\n",
            "4    rt glennfranco tech firm suing buzzfeed publis...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'data' is your cleaned text data and 'df' is your original DataFrame\n",
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_text, df['Sentiment'], test_size=0.2, random_state=42) # Example test_size and random_state\n"
      ],
      "metadata": {
        "id": "ETYwXLwH4oO5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Training and Evaluation\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyeazYYp40Jn",
        "outputId": "1eb964fe-0444-40c7-91f9-3ff8ebea649b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96    248563\n",
            "           1       0.94      0.91      0.93    121462\n",
            "\n",
            "    accuracy                           0.95    370025\n",
            "   macro avg       0.95      0.94      0.95    370025\n",
            "weighted avg       0.95      0.95      0.95    370025\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}